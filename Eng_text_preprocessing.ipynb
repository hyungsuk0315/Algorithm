{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Eng_text_preprocessing.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hyungsuk0315/Algorithm/blob/master/Eng_text_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szO1PXj30d0w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "017de904-199c-49b9-9a65-8c2f8f9e6a82"
      },
      "source": [
        "## 1.1 실습용 영문기사 수집\n",
        "#온라인 기사를 바로 수집하여 실습데이터로 사용\n",
        "\n",
        "#https://www.forbes.com/sites/adrianbridgwater/2019/04/15/what-drove-the-ai-renaissance/\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "url = 'https://www.forbes.com/sites/adrianbridgwater/2019/04/15/what-drove-the-ai-renaissance/?ss=ai-big-data#45dd5dd61f25'\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.text,'html.parser')\n",
        "\n",
        "eng_news = soup.select('p') #[class=\"speakable-paragraph\"]')\n",
        "eng_text = eng_news[3].get_text()\n",
        "\n",
        "eng_text\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"And yes, she does mean everybody's job from yours to mine and onward to the role of grain farmers in Egypt, pastry chefs in Paris and dog walkers in Oregon i.e. every job. We will now be able to help direct all workers’ actions and behavior with a new degree of intelligence that comes from predictive analytics, all stemming from the AI engines we will now increasingly depend upon.\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SgGzgELAcHA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "715e8049-17ba-41a4-9749-40aef86f191d"
      },
      "source": [
        "#word_tokenize() : 마침표와 구두점(온점. 컴마, 물음표? 세미콜론; 느낌표! 등;)\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "token1 = word_tokenize('Barack Obama likes fried chicken very much')\n",
        "print(token1)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "['Barack', 'Obama', 'likes', 'fried', 'chicken', 'very', 'much']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqIs1qxEBnkp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "61177da0-8ef2-4732-d82a-317e877c1726"
      },
      "source": [
        "import nltk\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "token2 = WordPunctTokenizer().tokenize(eng_text)\n",
        "print(token2)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['And', 'yes', ',', 'she', 'does', 'mean', 'everybody', \"'\", 's', 'job', 'from', 'yours', 'to', 'mine', 'and', 'onward', 'to', 'the', 'role', 'of', 'grain', 'farmers', 'in', 'Egypt', ',', 'pastry', 'chefs', 'in', 'Paris', 'and', 'dog', 'walkers', 'in', 'Oregon', 'i', '.', 'e', '.', 'every', 'job', '.', 'We', 'will', 'now', 'be', 'able', 'to', 'help', 'direct', 'all', 'workers', '’', 'actions', 'and', 'behavior', 'with', 'a', 'new', 'degree', 'of', 'intelligence', 'that', 'comes', 'from', 'predictive', 'analytics', ',', 'all', 'stemming', 'from', 'the', 'AI', 'engines', 'we', 'will', 'now', 'increasingly', 'depend', 'upon', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQ20zZbbBUmP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "outputId": "550a6b10-6669-4b57-db7b-ace488d84918"
      },
      "source": [
        "#TreeBankWordTokenizer() : 정규표현식에 기반한 토큰화\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "token = TreebankWordTokenizer().tokenize(eng_text)\n",
        "print(token)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "['And', 'yes', ',', 'she', 'does', 'mean', 'everybody', \"'s\", 'job', 'from', 'yours', 'to', 'mine', 'and', 'onward', 'to', 'the', 'role', 'of', 'grain', 'farmers', 'in', 'Egypt', ',', 'pastry', 'chefs', 'in', 'Paris', 'and', 'dog', 'walkers', 'in', 'Oregon', 'i.e.', 'every', 'job.', 'We', 'will', 'now', 'be', 'able', 'to', 'help', 'direct', 'all', 'workers', '’', 'actions', 'and', 'behavior', 'with', 'a', 'new', 'degree', 'of', 'intelligence', 'that', 'comes', 'from', 'predictive', 'analytics', ',', 'all', 'stemming', 'from', 'the', 'AI', 'engines', 'we', 'will', 'now', 'increasingly', 'depend', 'upon', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tvOg29DMCe4i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "912ef438-65a9-477b-f6bc-8e046d941734"
      },
      "source": [
        ""
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Object `WordPuncTokenizer` not found.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n712NiRXCiQI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "1f7aa059-00fe-4ef1-d786-3b5e69289581"
      },
      "source": [
        "from nltk import pos_tag\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMmf9rLDFSD1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "2d18679d-dc03-4080-9e88-31e631e85470"
      },
      "source": [
        "taggedToken=pos_tag(token)\n",
        "print(taggedToken)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('And', 'CC'), ('yes', 'UH'), (',', ','), ('she', 'PRP'), ('does', 'VBZ'), ('mean', 'VB'), ('everybody', 'NN'), (\"'s\", 'POS'), ('job', 'NN'), ('from', 'IN'), ('yours', 'NNS'), ('to', 'TO'), ('mine', 'VB'), ('and', 'CC'), ('onward', 'VB'), ('to', 'TO'), ('the', 'DT'), ('role', 'NN'), ('of', 'IN'), ('grain', 'NN'), ('farmers', 'NNS'), ('in', 'IN'), ('Egypt', 'NNP'), (',', ','), ('pastry', 'NN'), ('chefs', 'NNS'), ('in', 'IN'), ('Paris', 'NNP'), ('and', 'CC'), ('dog', 'NN'), ('walkers', 'NNS'), ('in', 'IN'), ('Oregon', 'NNP'), ('i.e.', 'NN'), ('every', 'DT'), ('job.', 'NN'), ('We', 'PRP'), ('will', 'MD'), ('now', 'RB'), ('be', 'VB'), ('able', 'JJ'), ('to', 'TO'), ('help', 'VB'), ('direct', 'VB'), ('all', 'DT'), ('workers', 'NNS'), ('’', 'VBP'), ('actions', 'NNS'), ('and', 'CC'), ('behavior', 'NN'), ('with', 'IN'), ('a', 'DT'), ('new', 'JJ'), ('degree', 'NN'), ('of', 'IN'), ('intelligence', 'NN'), ('that', 'WDT'), ('comes', 'VBZ'), ('from', 'IN'), ('predictive', 'JJ'), ('analytics', 'NNS'), (',', ','), ('all', 'DT'), ('stemming', 'VBG'), ('from', 'IN'), ('the', 'DT'), ('AI', 'NNP'), ('engines', 'VBZ'), ('we', 'PRP'), ('will', 'MD'), ('now', 'RB'), ('increasingly', 'RB'), ('depend', 'VBP'), ('upon', 'NN'), ('.', '.')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "csszgJImHWmM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "bb5ce8e8-baad-4887-9e05-087c8ae9f874"
      },
      "source": [
        "nltk.download('words')\n",
        "nltk.download('maxent_ne_chunker')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtWGJcfJFYet",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk import ne_chunk"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFdPQIEzHNf0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1385
        },
        "outputId": "7697aa9c-d527-40f3-ab6d-cd4ed8f6c9bd"
      },
      "source": [
        "neToken = ne_chunk(taggedToken)\n",
        "print(neToken)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(S\n",
            "  And/CC\n",
            "  yes/UH\n",
            "  ,/,\n",
            "  she/PRP\n",
            "  does/VBZ\n",
            "  mean/VB\n",
            "  everybody/NN\n",
            "  's/POS\n",
            "  job/NN\n",
            "  from/IN\n",
            "  yours/NNS\n",
            "  to/TO\n",
            "  mine/VB\n",
            "  and/CC\n",
            "  onward/VB\n",
            "  to/TO\n",
            "  the/DT\n",
            "  role/NN\n",
            "  of/IN\n",
            "  grain/NN\n",
            "  farmers/NNS\n",
            "  in/IN\n",
            "  (GPE Egypt/NNP)\n",
            "  ,/,\n",
            "  pastry/NN\n",
            "  chefs/NNS\n",
            "  in/IN\n",
            "  (GPE Paris/NNP)\n",
            "  and/CC\n",
            "  dog/NN\n",
            "  walkers/NNS\n",
            "  in/IN\n",
            "  (GPE Oregon/NNP)\n",
            "  i.e./NN\n",
            "  every/DT\n",
            "  job./NN\n",
            "  We/PRP\n",
            "  will/MD\n",
            "  now/RB\n",
            "  be/VB\n",
            "  able/JJ\n",
            "  to/TO\n",
            "  help/VB\n",
            "  direct/VB\n",
            "  all/DT\n",
            "  workers/NNS\n",
            "  ’/VBP\n",
            "  actions/NNS\n",
            "  and/CC\n",
            "  behavior/NN\n",
            "  with/IN\n",
            "  a/DT\n",
            "  new/JJ\n",
            "  degree/NN\n",
            "  of/IN\n",
            "  intelligence/NN\n",
            "  that/WDT\n",
            "  comes/VBZ\n",
            "  from/IN\n",
            "  predictive/JJ\n",
            "  analytics/NNS\n",
            "  ,/,\n",
            "  all/DT\n",
            "  stemming/VBG\n",
            "  from/IN\n",
            "  the/DT\n",
            "  (ORGANIZATION AI/NNP)\n",
            "  engines/VBZ\n",
            "  we/PRP\n",
            "  will/MD\n",
            "  now/RB\n",
            "  increasingly/RB\n",
            "  depend/VBP\n",
            "  upon/NN\n",
            "  ./.)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNzOAl5mHRyT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "outputId": "1037c972-03fc-45fe-bdb2-bf010b7510d0"
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "pa = PorterStemmer()\n",
        "\n",
        "print(\"running -> \" + pa.stem('running'))\n",
        "print(\"beautiful -> \" + pa.stem('beautiful'))\n",
        "print(\"believes -> \" + pa.stem('beileves'))\n",
        "print(\"using -> \" + pa.stem('using'))\n",
        "print(\"conversation -> \" + pa.stem('conversation'))\n",
        "print(\"organization -> \" + pa.stem('organization'))\n",
        "print(\"studies -> \" + pa.stem('studies'))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "running -> run\n",
            "beautiful -> beauti\n",
            "believes -> beilev\n",
            "using -> use\n",
            "conversation -> convers\n",
            "organization -> organ\n",
            "studies -> studi\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSfqxPPdH5TS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        },
        "outputId": "bfd3f099-8651-4f4a-8e60-b116de6b3001"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "wl = WordNetLemmatizer()\n",
        "nltk.download('wordnet')\n",
        "print(\"running -> \" + wl.lemmatize('running'))\n",
        "print(\"beautiful -> \" + wl.lemmatize('beautiful'))\n",
        "print(\"believes -> \" + wl.lemmatize('beileves'))\n",
        "print(\"using -> \" + wl.lemmatize('using'))\n",
        "print(\"conversation -> \" + wl.lemmatize('conversation'))\n",
        "print(\"organization -> \" + wl.lemmatize('organization'))\n",
        "print(\"studies -> \" + wl.lemmatize('studies'))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "running -> running\n",
            "beautiful -> beautiful\n",
            "believes -> beileves\n",
            "using -> using\n",
            "conversation -> conversation\n",
            "organization -> organization\n",
            "studies -> study\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gC8IOyvbJOO_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stopPos=['IN','CC','UH', 'TO', 'MD', 'DT', 'VBZ', 'VBP']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqT7pqXPKZFQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1097
        },
        "outputId": "cbfd8ee2-7f7f-425a-a83b-6e600d92cb85"
      },
      "source": [
        "from collections import Counter\n",
        "Counter(taggedToken).most_common()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[((',', ','), 3),\n",
              " (('from', 'IN'), 3),\n",
              " (('to', 'TO'), 3),\n",
              " (('and', 'CC'), 3),\n",
              " (('in', 'IN'), 3),\n",
              " (('the', 'DT'), 2),\n",
              " (('of', 'IN'), 2),\n",
              " (('will', 'MD'), 2),\n",
              " (('now', 'RB'), 2),\n",
              " (('all', 'DT'), 2),\n",
              " (('And', 'CC'), 1),\n",
              " (('yes', 'UH'), 1),\n",
              " (('she', 'PRP'), 1),\n",
              " (('does', 'VBZ'), 1),\n",
              " (('mean', 'VB'), 1),\n",
              " (('everybody', 'NN'), 1),\n",
              " ((\"'s\", 'POS'), 1),\n",
              " (('job', 'NN'), 1),\n",
              " (('yours', 'NNS'), 1),\n",
              " (('mine', 'VB'), 1),\n",
              " (('onward', 'VB'), 1),\n",
              " (('role', 'NN'), 1),\n",
              " (('grain', 'NN'), 1),\n",
              " (('farmers', 'NNS'), 1),\n",
              " (('Egypt', 'NNP'), 1),\n",
              " (('pastry', 'NN'), 1),\n",
              " (('chefs', 'NNS'), 1),\n",
              " (('Paris', 'NNP'), 1),\n",
              " (('dog', 'NN'), 1),\n",
              " (('walkers', 'NNS'), 1),\n",
              " (('Oregon', 'NNP'), 1),\n",
              " (('i.e.', 'NN'), 1),\n",
              " (('every', 'DT'), 1),\n",
              " (('job.', 'NN'), 1),\n",
              " (('We', 'PRP'), 1),\n",
              " (('be', 'VB'), 1),\n",
              " (('able', 'JJ'), 1),\n",
              " (('help', 'VB'), 1),\n",
              " (('direct', 'VB'), 1),\n",
              " (('workers', 'NNS'), 1),\n",
              " (('’', 'VBP'), 1),\n",
              " (('actions', 'NNS'), 1),\n",
              " (('behavior', 'NN'), 1),\n",
              " (('with', 'IN'), 1),\n",
              " (('a', 'DT'), 1),\n",
              " (('new', 'JJ'), 1),\n",
              " (('degree', 'NN'), 1),\n",
              " (('intelligence', 'NN'), 1),\n",
              " (('that', 'WDT'), 1),\n",
              " (('comes', 'VBZ'), 1),\n",
              " (('predictive', 'JJ'), 1),\n",
              " (('analytics', 'NNS'), 1),\n",
              " (('stemming', 'VBG'), 1),\n",
              " (('AI', 'NNP'), 1),\n",
              " (('engines', 'VBZ'), 1),\n",
              " (('we', 'PRP'), 1),\n",
              " (('increasingly', 'RB'), 1),\n",
              " (('depend', 'VBP'), 1),\n",
              " (('upon', 'NN'), 1),\n",
              " (('.', '.'), 1)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFaHSgeeKyto",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "877cbec3-1691-49dd-b622-f4dd5b845861"
      },
      "source": [
        "stopWord=[',','be','able']\n",
        "\n",
        "word=[]\n",
        "for tag in taggedToken:\n",
        "  if tag[1] not in stopPos:\n",
        "    if tag[0] not in stopWord:\n",
        "      word.append(tag[0])\n",
        "print(word)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['she', 'mean', 'everybody', \"'s\", 'job', 'yours', 'mine', 'onward', 'role', 'grain', 'farmers', 'Egypt', 'pastry', 'chefs', 'Paris', 'dog', 'walkers', 'Oregon', 'i.e.', 'job.', 'We', 'now', 'help', 'direct', 'workers', 'actions', 'behavior', 'new', 'degree', 'intelligence', 'that', 'predictive', 'analytics', 'stemming', 'AI', 'we', 'now', 'increasingly', 'upon', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GpwWrd3fL56f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}